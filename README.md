# ğŸš€ **Airflow ETL Pipeline with Postgres & NASA API**
<div align="center">

âœ¨ An end-to-end ETL pipeline built with Apache Airflow, Docker, PostgreSQL, and NASAâ€™s APOD API âœ¨
Extract â€¢ Transform â€¢ Load â€¢ Orchestrate â€” fully automated data workflows

ğŸ› ï¸ Apache Airflow â€¢ ğŸ˜ PostgreSQL â€¢ ğŸ³ Docker â€¢ ğŸŒŒ NASA APOD API â€¢ ğŸ Python

</div>

## ğŸŒŸ Overview

This project demonstrates a production-style ETL (Extract, Transform, Load) pipeline orchestrated using Apache Airflow.

The pipeline automatically retrieves data from NASAâ€™s Astronomy Picture of the Day (APOD) API, processes the response, and stores structured data into a PostgreSQL database.
All services run inside Docker containers, providing a reproducible and isolated environment.

This project showcases real-world data engineering workflows, including API ingestion, transformation logic, database loading, and orchestration with Airflow DAGs.


## ğŸ¯ Project Objectives

### âœ”ï¸ Build an automated ETL pipeline using Apache Airflow
### âœ”ï¸ Integrate an external REST API
### âœ”ï¸ Perform data transformation using Python
### âœ”ï¸ Load structured data into PostgreSQL
### âœ”ï¸ Use Docker for reproducible environments
### âœ”ï¸ Demonstrate Airflow hooks, operators, and DAG design

---

## ğŸ§© Core Components
### ğŸŒ€ Apache Airflow (Orchestration)

- Defines and schedules the ETL workflow using a DAG

- Manages task dependencies and execution order

- Provides monitoring, logging, retries, and failure handling

### ğŸŒŒ NASA APOD API (Data Source)

- The pipeline extracts data from NASAâ€™s Astronomy Picture of the Day API.

- Data is fetched daily using HTTP requests.

### ğŸ˜ PostgreSQL (Data Storage)

- Stores extracted and transformed data

- Runs inside a Docker container

- Uses Docker volumes for data persistence

- Interacted with using Airflowâ€™s PostgresHook

### ğŸ³ Docker & Docker Compose

- Containerizes: Apache Airflow and PostgreSQL

- Ensures consistent environments across machines

- Simplifies setup and deployment

---

## ğŸ”„ ETL Workflow
### 1ï¸âƒ£ Extract

- Uses Airflow SimpleHttpOperator

- Sends a GET request to the NASA APOD API

- Retrieves JSON response containing astronomy metadata

### 2ï¸âƒ£ Transform

- Implemented using Airflow TaskFlow API (@task)

- Parses and validates API response

- Extracts required fields

- Prepares data for database insertion

### 3ï¸âƒ£ Load

- Uses Airflow PostgresHook

- Inserts transformed data into PostgreSQL

- Automatically creates the target table if it does not exist

---

## â° Scheduling & Automation

- Pipeline runs on a daily schedule

- Task dependencies: Extract â†’ Transform â†’ Load order

- Airflow handles: Retries, Logging and Failure alerts

## ğŸ“ Project Structure

```bash

.
â”œâ”€â”€ dags/                  # Airflow DAG definitions
â”œâ”€â”€ tests/dags/            # DAG tests
â”œâ”€â”€ .astro/                # Astro CLI configuration
â”œâ”€â”€ Dockerfile             # Custom Airflow image
â”œâ”€â”€ docker-compose.yml     # Airflow + Postgres services
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ packages.txt           # System-level packages
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸš€ Getting Started
### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/kirantushar10/Apache-Airflow-ETL-With-Postgres-and-API.git
cd Apache-Airflow-ETL-With-Postgres-and-API
```

### 2ï¸âƒ£ Start services with Airflow
```bash
astro dev start
```

- This will start the project with docker.

### 3ï¸âƒ£ Access the Airflow UI
```bash
URL: http://localhost:8080
```

## ğŸ“œ License

This project is licensed under the GPL-3.0 license.

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
Feel free to fork the repository and submit a pull request.


